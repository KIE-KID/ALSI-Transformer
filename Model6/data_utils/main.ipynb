{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from utils_park import *\n",
    "from vocab import *\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rq1 = '../newsbt_data'\n",
    "VOCAB_SIZE = 40000 # 40000\n",
    "out_path = os.path.join(rq1, 'vocabulary/')\n",
    "\n",
    "PAD = '<pad>'\n",
    "UNK = '<unk>'\n",
    "SOS = '<start>'\n",
    "EOS = '<end>'\n",
    "\n",
    "PAD_ID = 0\n",
    "UNK_ID = 1\n",
    "SOS_ID = 2\n",
    "EOS_ID = 3\n",
    "\n",
    "train_newsbtcode_path = os.path.join(rq1,'train/newsbtcode')\n",
    "valid_newsbtcode_path = os.path.join(rq1,'valid/newsbtcode')\n",
    "test_newsbtcode_path = os.path.join(rq1,'test/newsbtcode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445812\n",
      "20000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "train_newsbtcode_lines = read_file(train_newsbtcode_path)\n",
    "test_newsbtcode_lines = read_file(test_newsbtcode_path)\n",
    "valid_newsbtcode_lines = read_file(valid_newsbtcode_path)\n",
    "print(len(train_newsbtcode_lines)) # 문장개수\n",
    "print(len(test_newsbtcode_lines)) # 문장개수\n",
    "print(len(valid_newsbtcode_lines)) # 문장개수\n",
    "\n",
    "# for line in isbtcode_lines[:3]:\n",
    "#     print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(files=\"../newsbt_data/train/newsbtcode\",\n",
    "                vocab_size=52000, min_frequency=1, special_tokens=[\"<pad>\",\"<unk>\", \"<start>\", \"<end>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer(BPE())\n",
    "trainer = BpeTrainer(vocab_size=52000,\n",
    "show_progress = True\n",
    "special_tokens = [\"<pad>\",\"<unk>\", \"<start>\", \"<end>\"])\n",
    "t.train(files=[\"../newsbt_data/train/newsbtcode\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(os.path.exists(\"./newsbt_voacb\") == False):\n",
    "    os.makedirs(\"./newsbt_voacb\")\n",
    "tokenizer.save_model(\"./newsbt_voacb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_model(\"../newsbt_data/vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer = BartTokenizer(vocab_file = \"./newsbt_vocab/vocab.json\",\n",
    "merges_file=\"./newsbt_vocab/merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer.pad_token='<pad>'\n",
    "my_tokenizer.unk_token='<unk>'\n",
    "my_tokenizer.bos_token='<start>'\n",
    "my_tokenizer.eos_token='<end>'\n",
    "\n",
    "my_tokenizer.pad_token_id=0\n",
    "my_tokenizer.unk_token_id=1\n",
    "my_tokenizer.bos_token_id=2\n",
    "my_tokenizer.eos_token_id=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokens(t, lines):\n",
    "    data = []\n",
    "    for i in lines:\n",
    "        data.append(t.encode(i)) # , max_length=256, truncation=True, pad_to_max_length=True\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_newsbtcode = make_tokens(my_tokenizer, train_newsbtcode_lines)\n",
    "test_newsbtcode = make_tokens(my_tokenizer, test_newsbtcode_lines)\n",
    "valid_newsbtcode = make_tokens(my_tokenizer, valid_newsbtcode_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_newsbtcode =make_tokens(my_tokenizer, test_newsbtcode_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3161,\n",
       " 375,\n",
       " 335,\n",
       " 405,\n",
       " 363,\n",
       " 1010,\n",
       " 748,\n",
       " 356,\n",
       " 281,\n",
       " 268,\n",
       " 591,\n",
       " 329,\n",
       " 651,\n",
       " 305,\n",
       " 320,\n",
       " 268,\n",
       " 591,\n",
       " 329,\n",
       " 563,\n",
       " 305,\n",
       " 282,\n",
       " 268,\n",
       " 518,\n",
       " 296,\n",
       " 1018,\n",
       " 278,\n",
       " 322,\n",
       " 268,\n",
       " 394,\n",
       " 296,\n",
       " 281,\n",
       " 268,\n",
       " 563,\n",
       " 305,\n",
       " 464,\n",
       " 286,\n",
       " 2680,\n",
       " 278,\n",
       " 302,\n",
       " 268,\n",
       " 41146,\n",
       " 278,\n",
       " 282,\n",
       " 268,\n",
       " 322,\n",
       " 268,\n",
       " 379,\n",
       " 296,\n",
       " 387,\n",
       " 66,\n",
       " 278,\n",
       " 290,\n",
       " 268,\n",
       " 323,\n",
       " 268,\n",
       " 405,\n",
       " 363,\n",
       " 677,\n",
       " 278,\n",
       " 330,\n",
       " 286,\n",
       " 8428,\n",
       " 19593,\n",
       " 1738,\n",
       " 2412,\n",
       " 278,\n",
       " 302,\n",
       " 268,\n",
       " 434,\n",
       " 296,\n",
       " 302,\n",
       " 268,\n",
       " 380,\n",
       " 1673,\n",
       " 1055,\n",
       " 3092,\n",
       " 5100,\n",
       " 278,\n",
       " 281,\n",
       " 268,\n",
       " 282,\n",
       " 268,\n",
       " 290,\n",
       " 268,\n",
       " 677,\n",
       " 278,\n",
       " 1082,\n",
       " 286,\n",
       " 677,\n",
       " 879,\n",
       " 278,\n",
       " 281,\n",
       " 268,\n",
       " 651,\n",
       " 305,\n",
       " 282,\n",
       " 268,\n",
       " 290,\n",
       " 268,\n",
       " 677,\n",
       " 278,\n",
       " 1082,\n",
       " 286,\n",
       " 677,\n",
       " 879,\n",
       " 278,\n",
       " 281,\n",
       " 268,\n",
       " 563,\n",
       " 305,\n",
       " 282,\n",
       " 268,\n",
       " 290,\n",
       " 268,\n",
       " 379,\n",
       " 296,\n",
       " 677,\n",
       " 278,\n",
       " 290,\n",
       " 268,\n",
       " 323,\n",
       " 268,\n",
       " 3161]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_newsbtcode[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20public Modifier int BasicType entrySize MethodDeclaration ( Separator Object ReferenceType key FormalParameter, Separator Object ReferenceType value FormalParameter ) Separator throws Keyword IllegalArgumentException Identifier { Separator if Keyword ( Separator value FormalParameter == Operator Token Identifier. Separator TOMBSTONE Identifier ) Separator { Separator return Keyword NUM_ Identifier ; Separator } Separator int BasicType size Identifier = Operator HeapLRUCapacityController Identifier. Separator this Keyword. Separator getPerEntryOverhead Identifier ( Separator ) Separator ; Separator size Identifier += Operator sizeof Identifier ( Separator key FormalParameter ) Separator ; Separator size Identifier += Operator sizeof Identifier ( Separator value FormalParameter ) Separator ; Separator return Keyword size Identifier ; Separator } Separator20'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tokenizer.decode(test_newsbtcode[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out path\n",
    "train_out_path = '../newsbt_data/train/'\n",
    "test_out_path = '../newsbt_data/test/'\n",
    "valid_out_path = '../newsbt_data/valid/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_idx_file(train_out_path, 'newsbtcode_idx', train_newsbtcode)\n",
    "save_idx_file(test_out_path, 'newsbtcode_idx', test_newsbtcode)\n",
    "save_idx_file(valid_out_path, 'newsbtcode_idx', valid_newsbtcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(target)\n",
    "tokenizer.decode(summary_text_ids[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "caf1c2fcf97217de91eafa76b907d50f9ea378f5ffbee7f571142d119bb6a771"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
