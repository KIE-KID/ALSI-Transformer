{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import javalang\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import Text\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rq1 = './data_RQ1/'\n",
    "VOCAB_SIZE = 30000\n",
    "\n",
    "PAD_ = '<pad>'\n",
    "UNK = '<unk>'\n",
    "SOS = '<start>'\n",
    "EOS = '<end>'\n",
    "\n",
    "PAD_ID = 0\n",
    "UNK_ID = 1\n",
    "SOS_ID = 2\n",
    "EOS_ID = 3\n",
    "\n",
    "train_code_path = os.path.join(rq1, 'train/train.token.code')\n",
    "train_nl_path = os.path.join(rq1,'train/train.token.nl')\n",
    "valid_code_path = os.path.join(rq1, 'valid/valid.token.code')\n",
    "valid_nl_path = os.path.join(rq1, 'valid/valid.token.nl')\n",
    "test_code_path = os.path.join(rq1, 'test/test.token.code')\n",
    "test_nl_path =  os.path.join(rq1, 'test/test.token.nl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT_RX = re.compile(\"(?<!:)\\\\/\\\\/.*|\\\\/\\\\*(\\\\s|.)*?\\\\*\\\\/\", re.MULTILINE)\n",
    "\n",
    "def process_source(code):\n",
    "    code = code.replace('\\n',' ').strip()\n",
    "    tokens = list(javalang.tokenizer.tokenize(code))\n",
    "    tks = []\n",
    "    for tk in tokens:\n",
    "        if tk.__class__.__name__ == 'String' or tk.__class__.__name__ == 'Character':\n",
    "            tks.append('STR_')\n",
    "        elif 'Integer' in tk.__class__.__name__ or 'FloatingPoint' in tk.__class__.__name__:\n",
    "            tks.append('NUM_')\n",
    "        elif tk.__class__.__name__ == 'Boolean':\n",
    "            tks.append('BOOL_')\n",
    "        else:\n",
    "            tks.append(tk.value)\n",
    "    return \" \".join(tks)\n",
    "\n",
    "def hump2underline(hunp_str):\n",
    "    '''\n",
    "    밑줄을 칠 CamelCase 문자열\n",
    "    :param hunp_str: CamelCase 밑줄\n",
    "    :return: 모두 소문자로 된 밑줄이 그어진 문자열\n",
    "    '''\n",
    "    p = re.compile(r'([a-z]|\\d)([A-Z])') # 일반 일치, 소문자와 대문자의 경계 일치\n",
    "    sub = re.sub(p, r'\\1 \\2', hunp_str).lower() # 여기서 두 번째 매개변수는 일반 그룹화된 역참조를 사용\n",
    "    return sub\n",
    "\n",
    "# code processing\n",
    "def split_code(lines):\n",
    "    result = []\n",
    "    for line in lines:\n",
    "        code = COMMENT_RX.sub('', line)\n",
    "        processed_code = process_source(code)\n",
    "        code_seq = ' '.join([hump2underline(i) for i in processed_code.split()])\n",
    "        result.append(code_seq)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vocab 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_token_instance(lines):\n",
    "    tokens = []\n",
    "    for line in lines:\n",
    "        tokens.extend(word_tokenize(str(line))) # 문장을 단어로 tokenize\n",
    "\n",
    "    t = Text(tokens) # Token을 기반으로 정보를 담기 위한 인스턴스\n",
    "\n",
    "    return t\n",
    "\n",
    "# print(len(nl_t), len(set(nl_t))) # 191785개의 단어가 t 객체에 있음, 중복 제거시 58422\n",
    "def make_vocab(lines, dic_type):\n",
    "    t = make_token_instance(lines)\n",
    "    vocab = t.vocab().most_common(VOCAB_SIZE) # 상위 vocab_size개의 단어만 보존 \n",
    "\n",
    "    if dic_type == 'nl':\n",
    "        word_to_index = {word[0] : index + 4 for index, word in enumerate(vocab)} # 각 단어에 대해 고유한 정수 부여하기(indexing)\n",
    "\n",
    "        word_to_index['<pad>'] = 0\n",
    "        word_to_index['<unk>'] = 1\n",
    "        word_to_index['<start>'] = 2\n",
    "        word_to_index['<end>'] = 3\n",
    "    else:\n",
    "        word_to_index = {word[0] : index + 2 for index, word in enumerate(vocab)} # 각 단어에 대해 고유한 정수 부여하기(indexing)\n",
    "\n",
    "        word_to_index['<pad>'] = 0\n",
    "        word_to_index['<unk>'] = 1\n",
    "\n",
    "    # sorted_nl_dic = sorted(word_to_index.items(), key=lambda x:x[1]) \n",
    "    vocab_list = [x[0] for x in sorted(word_to_index.items(), key=lambda x:x[1])] # value 기준으로 정렬하고 키값(토큰)만 추출\n",
    "    return vocab_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Vocab 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name = train_nl_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445812\n",
      "create native global variables from the modules the returned object can be reused for different instances of environments .\n",
      "\n",
      "just a simple check to see if the x y pair actually fits into the pixel array .\n",
      "\n",
      "patches the given resource and will also remove private properties if it is an external call based upon context .\n",
      "\n",
      "to zoom out .\n",
      "\n",
      "creates a new plot .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(f_name, 'r', encoding='utf-8') as f:\n",
    "    code_lines = f.readlines()\n",
    "\n",
    "print(len(code_lines)) # 문장개수\n",
    "\n",
    "for line in code_lines[:5]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_tokens = split_code(code_lines)\n",
    "code_vocab = make_vocab(code_tokens, \"code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<unk>', '.', 'the', 'a', 'to', 'of', 'this', 'and', 'is']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data_RQ1/vocab_park/'\n",
    "if not os.path.isdir(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "with open('vocab.code', 'w', encoding='utf-8') as f:\n",
    "    for i in code_vocab:\n",
    "        f.write(i+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nl vocab 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name = train_nl_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f_name, 'r', encoding='utf-8') as f:\n",
    "    nl_lines = f.readlines()\n",
    "\n",
    "print(len(nl_lines)) # 문장개수\n",
    "\n",
    "for line in nl_lines[:5]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_vocab = make_vocab(nl, \"nl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lines[:5]:\n",
    "    print(sent_tokenize(str(i)))\n",
    "for i in lines[:5]:\n",
    "    print(word_tokenize(str(i)))    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab 불러와서 문장 indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = [] \n",
    "for line in tokenized: #입력 데이터에서 1줄씩 문장을 읽음 \n",
    "    temp = [] for w in line: #각 줄에서 1개씩 글자를 읽음 \n",
    "    try: \n",
    "        temp.append(word_to_index[w]) # 글자를 해당되는 정수로 변환 \n",
    "    except KeyError: # 단어 집합에 없는 단어일 경우 unk로 대체된다. \n",
    "        temp.append(word_to_index['unk']) # unk의 인덱스로 변환 \n",
    "    encoded.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_source(file_name, save_file):\n",
    "    with open(file_name, 'r', encoding='utf-8') as source:\n",
    "        lines = source.readlines()\n",
    "    with open(save_file, 'w+', encoding='utf-8') as save:\n",
    "        for line in lines:\n",
    "            code = line.strip()\n",
    "            tokens = list(javalang.tokenizer.tokenize(code))\n",
    "            tks = []\n",
    "            for tk in tokens:\n",
    "                if tk.__class__.__name__ == 'String' or tk.__class__.__name__ == 'Character':\n",
    "                    tks.append('STR_')\n",
    "                elif 'Integer' in tk.__class__.__name__ or 'FloatingPoint' in tk.__class__.__name__:\n",
    "                    tks.append('NUM_')\n",
    "                elif tk.__class__.__name__ == 'Boolean':\n",
    "                    tks.append('BOOL_')\n",
    "                else:\n",
    "                    tks.append(tk.value)\n",
    "            save.write(\" \".join(tks) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vocabulary(vocabulary_path):\n",
    "    \"\"\"\n",
    "    Initialize vocabulary from file.\n",
    "\n",
    "    We assume the vocabulary is stored one-item-per-line, so a file:\n",
    "      dog\n",
    "      cat\n",
    "    will result in a vocabulary {'dog': 0, 'cat': 1}, and a reversed vocabulary ['dog', 'cat'].\n",
    "\n",
    "    :param vocabulary_path: path to the file containing the vocabulary.\n",
    "    :return:\n",
    "      the vocabulary (a dictionary mapping string to integers), and\n",
    "      the reversed vocabulary (a list, which reverses the vocabulary mapping).\n",
    "    \"\"\"\n",
    "    if os.path.exists(vocabulary_path):\n",
    "        rev_vocab = []\n",
    "        with open(vocabulary_path) as f:\n",
    "            rev_vocab.extend(f.readlines())\n",
    "        rev_vocab = [line.rstrip('\\n') for line in rev_vocab]\n",
    "        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
    "        return namedtuple('vocab', 'vocab reverse')(vocab, rev_vocab)\n",
    "    else:\n",
    "        raise ValueError(\"vocabulary file %s not found\", vocabulary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def read_vocab(self):\n",
    "        # don't try reading vocabulary for encoders that take pre-computed features\n",
    "        self.vocabs = [\n",
    "            None if binary else utils.initialize_vocabulary(vocab_path)\n",
    "            for vocab_path, binary in zip(self.filenames.vocab, self.binary)\n",
    "            ]\n",
    "        self.src_vocab, self.trg_vocab = self.vocabs[:len(self.src_ext)], self.vocabs[len(self.src_ext):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./vocabulary/nl', 'r', encoding='utf-8')\n",
    "s = f.readlines()\n",
    "f.close()\n",
    "dic_word = {}\n",
    "key = 0\n",
    "for c in s:\n",
    "    dic_word[key] = c.strip()\n",
    "    key += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(paths, extensions, vocabs, max_size=None, character_level=None, sort_by_length=False,\n",
    "                 max_seq_len=None, from_position=None, binary=None, use_unknown=True):\n",
    "    data_set = []\n",
    "\n",
    "    if from_position is not None:\n",
    "        debug('reading from position: {}'.format(from_position))\n",
    "\n",
    "    line_reader = read_lines_from_position(paths, from_position=from_position, binary=binary)\n",
    "    character_level = character_level or {}\n",
    "\n",
    "    positions = None\n",
    "\n",
    "    for inputs, positions in line_reader:\n",
    "        if len(data_set) > 0 and len(data_set) % 100000 == 0:\n",
    "            debug(\"  lines read: {}\".format(len(data_set)))\n",
    "        lines = [\n",
    "            input_ if binary_ else\n",
    "            sentence_to_token_ids(input_, vocab.vocab,ext, character_level=character_level.get(ext), use_unknown=use_unknown)\n",
    "            for input_, vocab, binary_, ext in zip(inputs, vocabs, binary, extensions)\n",
    "        ]\n",
    "\n",
    "        if not all(lines):  # skip empty inputs\n",
    "            continue\n",
    "        # skip lines that are too long\n",
    "        if max_seq_len and any(len(line) > max_seq_len[ext] for line, ext in zip(lines, extensions)):\n",
    "            continue\n",
    "\n",
    "        data_set.append(lines)\n",
    "\n",
    "        if max_size and len(data_set) >= max_size:\n",
    "            break\n",
    "\n",
    "    debug('files: {}'.format(' '.join(paths)))\n",
    "    debug('lines reads: {}'.format(len(data_set)))\n",
    "\n",
    "    if sort_by_length:\n",
    "        data_set.sort(key=lambda lines: list(map(len, lines)))\n",
    "\n",
    "    return data_set, positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_token_ids(sentence, vocabulary, ext, character_level=False):\n",
    "    \"\"\"\n",
    "    Convert a string to list of integers representing token-ids.\n",
    "\n",
    "    For example, a sentence \"I have a dog\" may become tokenized into\n",
    "    [\"I\", \"have\", \"a\", \"dog\"] and with vocabulary {\"I\": 1, \"have\": 2,\n",
    "    \"a\": 4, \"dog\": 7\"} this function will return [1, 2, 4, 7].\n",
    "\n",
    "    :param sentence: a string, the sentence to convert to token-ids\n",
    "    :param vocabulary: a dictionary mapping tokens to integers\n",
    "    :param character_level: treat sentence as a string of characters, and\n",
    "        not as a string of words\n",
    "    :return: a list of integers, the token-ids for the sentence.\n",
    "    \"\"\"\n",
    "    sentence = sentence.strip()\n",
    "    sentence = sentence.rstrip('\\n') if character_level else sentence.split(' ')\n",
    "    if ext =='nl':\n",
    "        use_unknown=True\n",
    "    if use_unknown:\n",
    "        return [vocabulary.get(w, UNK_ID) for w in sentence]\n",
    "    else:\n",
    "        tks = []\n",
    "        for w in sentence:\n",
    "            if w not in vocabulary:\n",
    "                w = w.split('_')[0]\n",
    "            tks.append(vocabulary[w])\n",
    "        return tks"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "caf1c2fcf97217de91eafa76b907d50f9ea378f5ffbee7f571142d119bb6a771"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
